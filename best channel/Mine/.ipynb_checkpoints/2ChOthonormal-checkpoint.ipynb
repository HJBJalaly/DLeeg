{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mne in /home/hamed/anaconda3/envs/mne/lib/python3.6/site-packages (0.16.1)\n",
      "\u001b[31mmkl-random 1.0.1 requires cython, which is not installed.\u001b[0m\n",
      "\u001b[31mmkl-fft 1.0.0 requires cython, which is not installed.\u001b[0m\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.2.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install mne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 203
    },
    "colab_type": "code",
    "id": "ugNHdREkvO6h",
    "outputId": "0aa4ea5b-11b4-4909-e640-42366a31fa0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1 channel :  Cz (index =10)\n",
      "\n",
      "layers: 3 conv2D(3,3) & maxpooling(2,2)\n",
      "epoch: 20 \n",
      "batch size: 20\n",
      "\n",
      "test acc: 0.7886\n",
      "test loss: 0.6802\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "1 channel :  Cz (index =10)\n",
    "\n",
    "layers: 3 conv2D(3,3) & maxpooling(2,2)\n",
    "epoch: 20 \n",
    "batch size: 20\n",
    "\n",
    "test acc: 0.7886\n",
    "test loss: 0.6802\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import mne\n",
    "from mne.preprocessing import create_ecg_epochs, create_eog_epochs\n",
    "from mne import io\n",
    "from mne import viz\n",
    "#from mne.datasets import testing\n",
    "from mne import Epochs, io, pick_types\n",
    "from mne.event import define_target_events\n",
    "from mne.time_frequency import psd_welch\n",
    "print(__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "subject_number=20\n",
    "task_number=1\n",
    "task_time = 60\n",
    "sampel_number_per_sec =  160 # sampel rate\n",
    "total_sampel_number =  sampel_number_per_sec *task_time # 60*160\n",
    "sample_shift = 4 #step len\n",
    "window_len= 20\n",
    "list_channel = [10,11] # channel = Cz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EMBxiZ8KvXUP",
    "outputId": "57886af8-899c-4083-ea24-df239bacff67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S001/S001R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S001/S001R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S002/S002R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S002/S002R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S003/S003R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S003/S003R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S004/S004R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S004/S004R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S005/S005R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S005/S005R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S006/S006R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S006/S006R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S007/S007R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S007/S007R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S008/S008R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S008/S008R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S009/S009R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S009/S009R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S010/S010R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S010/S010R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S011/S011R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S011/S011R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S012/S012R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S012/S012R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S013/S013R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S013/S013R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S014/S014R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9599  =      0.000 ...    59.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S014/S014R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S015/S015R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S015/S015R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S016/S016R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S016/S016R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S017/S017R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S017/S017R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S018/S018R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S018/S018R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S019/S019R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S019/S019R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S020/S020R01.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n",
      "Extracting EDF parameters from /home/hamed/mne_data/MNE-eegbci-data/physiobank/database/eegmmidb/S020/S020R02.edf...\n",
      "EDF file detected\n",
      "EDF annotations detected (consider using raw.find_edf_events() to extract them)\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 9759  =      0.000 ...    60.994 secs...\n"
     ]
    }
   ],
   "source": [
    "# load dataset in array\n",
    "list_raw_fnames = [[0]*2]*subject_number\n",
    "for x in range(subject_number):\n",
    "    list_raw_fnames[x] = mne.datasets.eegbci.load_data(x+1,[1,2])\n",
    "\n",
    "list_rawdata1 = np.zeros((subject_number,2), dtype='object')\n",
    "list_rawdata = np.zeros((subject_number,2), dtype='object')\n",
    "\n",
    "for i in range(subject_number):\n",
    "    for j in range(2):\n",
    "        list_rawdata[i][j] = mne.io.read_raw_edf(list_raw_fnames[i][j], preload=True)\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "lltZvYULvrvA",
    "outputId": "70c7b7f0-2f98-471f-9382-cbbb1bff1130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample: 1757\n",
      "Test sample: 585\n",
      "train image dims: (35140, 2, 21, 160)\n"
     ]
    }
   ],
   "source": [
    "# img for each subject\n",
    "subject_img_number = math.floor((total_sampel_number - sampel_number_per_sec) / sample_shift) +1 - window_len +1\n",
    "test_start_index=subject_img_number - math.floor(subject_img_number/4) \n",
    "\n",
    "train_number =  test_start_index \n",
    "test_number = subject_img_number - test_start_index\n",
    "print(\"Train sample:\",train_number)\n",
    "print(\"Test sample:\",test_number)\n",
    "\n",
    "ch_number = len(list_channel)\n",
    "\n",
    "train_img = np.zeros((train_number*subject_number, ch_number, window_len + 1, 160), dtype = float)\n",
    "train_label =[]\n",
    "\n",
    "test_img = np.zeros((test_number*subject_number, ch_number, window_len, 160), dtype = float)\n",
    "test_label =[]\n",
    "\n",
    "print(\"train image dims:\",train_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2NuDhT0Y1Fqa"
   },
   "outputs": [],
   "source": [
    "def normalize_channel_data(ch , i, ch_min, ch_max):\n",
    "  ch = ((ch - ch_min[i]) / (ch_max[i] - ch_min[i] ))\n",
    "  return ch\n",
    "\n",
    "def ProjectionVector(VecA,VecB):\n",
    "    Projeted_VecB_on_VecA=np.dot(VecA,VecB)/np.dot(VecA,VecA)*VecA\n",
    "    return Projeted_VecB_on_VecA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MesamNSb1IGo"
   },
   "outputs": [],
   "source": [
    "# Make a copy of main data\n",
    "list_rawdataTemp=list_rawdata.copy()\n",
    "\n",
    "for s in range(subject_number):\n",
    "    \n",
    "    DataChannelsRaw, times =list_rawdataTemp[s][0][:64,:9600]\n",
    "    \n",
    "    # find min/max per channel\n",
    "    ch_max =[]\n",
    "    ch_min =[]\n",
    "    for ch in list_channel:\n",
    "      #print(ch)  \n",
    "      ch_max = np.append(ch_max, max(DataChannelsRaw[ch])) # max for each cannel\n",
    "      ch_min = np.append(ch_min, min(DataChannelsRaw[ch])) # min for each cannel\n",
    "    \n",
    "    # Normalize Channels\n",
    "    DataChannelsNormal=np.zeros((len(list_channel),times.size),dtype = float)\n",
    "    for i in range(len(list_channel)):\n",
    "        #print(i)\n",
    "        DataChannelsNormal[i]=normalize_channel_data(DataChannelsRaw[list_channel[i]].copy(), i, ch_min, ch_max)\n",
    "    \n",
    "    # Orthogonal Second Channel\n",
    "    DataChannelsOrt=np.zeros((len(list_channel),times.size),dtype = float)\n",
    "    DataChannelsOrt[0]=DataChannelsNormal[0]\n",
    "    DataChannelsOrt[1]=DataChannelsNormal[1]-ProjectionVector(DataChannelsNormal[0],DataChannelsNormal[1])\n",
    "    \n",
    "    for j in range (subject_img_number):\n",
    "        for i in range(len(list_channel)): # فقط کانال های با اندیس مشخص را دربرمیگیرد\n",
    "            for z in range (window_len):\n",
    "                #rawdataChannels, times =list_rawdata[s][0][index, (j+z)*sample_shift:sampel_number_per_sec+(j+z)*sample_shift]\n",
    "                ExtractedData=DataChannelsOrt[i,0+(j+z)*sample_shift:sampel_number_per_sec+(j+z)*sample_shift]\n",
    "\n",
    "\n",
    "                # normalize method\n",
    "                #rawdataChannels = normalize_channel_data(rawdataChannels, i, ch_min, ch_max)\n",
    "                if j <test_start_index :\n",
    "                    train_img[s*train_number + j][i][z] = ExtractedData\n",
    "                    if z==window_len-1 :\n",
    "                        train_img[s*train_number + j][i][z+1] = s\n",
    "                      \n",
    "                      \n",
    "                else:\n",
    "                     test_img[s*test_number + j - test_start_index][i][z] = ExtractedData\n",
    "            #i+=1            \n",
    "                    \n",
    "              \n",
    "        if j >=test_start_index :\n",
    "            test_label = np.append(test_label, (s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Make a copy of main data\n",
    "# list_rawdataTemp=list_rawdata.copy()\n",
    "\n",
    "# for s in range(subject_number):\n",
    "    \n",
    "#     # find min/max per channel\n",
    "#     ch_max =[]\n",
    "#     ch_min =[]\n",
    "#     for ch in list_channel:\n",
    "#       rawdataChannels_t, times_t =list_rawdataTemp[s][0][ch,:9600]\n",
    "#       #print(rawdataChannels_t)\n",
    "#       ch_max = np.append(ch_max, max(rawdataChannels_t[0])) # max for each cannel\n",
    "#       ch_min = np.append(ch_min, min(rawdataChannels_t[0])) # min for each cannel\n",
    "    \n",
    "#     # Normalize Channels\n",
    "#     DataChannelsNormal=np.zeros((len(list_channel),times.size),dtype = float)\n",
    "#     for i in range(len(list_channel)):\n",
    "#         DataChannelsNormal[i]=normalize_channel_data(DataChannelsRaw_t[i], i, ch_min, ch_max)\n",
    "    \n",
    "    \n",
    "#     for j in range (subject_img_number):\n",
    "#         i = 0\n",
    "#         for index in list_channel: # فقط کانال های با اندیس مشخص را دربرمیگیرد\n",
    "            \n",
    "#             for z in range (window_len):\n",
    "#                 rawdataChannels, times =list_rawdata[s][0][index, (j+z)*sample_shift:sampel_number_per_sec+(j+z)*sample_shift]\n",
    "\n",
    "#                       # normalize method\n",
    "#                 rawdataChannels = normalize_channel_data(rawdataChannels, i, ch_min, ch_max)\n",
    "#                 if j <test_start_index :\n",
    "#                     train_img[s*train_number + j][i][z] = rawdataChannels\n",
    "#                     if z==window_len-1 :\n",
    "#                       train_img[s*train_number + j][i][z+1] = s\n",
    "                      \n",
    "                      \n",
    "#                 else:\n",
    "#                     test_img[s*test_number + j - test_start_index][i][z] = rawdataChannels\n",
    "#             i+=1            \n",
    "                    \n",
    "              \n",
    "#         if j >=test_start_index :\n",
    "#           test_label = np.append(test_label, (s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FRJrmQA61TUI"
   },
   "outputs": [],
   "source": [
    "#shuffle train img array\n",
    "train_img_shuffle = train_img.copy()\n",
    "\n",
    "np.random.shuffle(train_img_shuffle)\n",
    "\n",
    "\n",
    "train_img_2 = np.zeros((train_number*subject_number, ch_number, window_len, 160), dtype = float)\n",
    "\n",
    "# img label \n",
    "train_img_shuffle_len =len(train_img_shuffle)\n",
    "for i in range(train_img_shuffle_len):\n",
    "    train_label = np.append(train_label, (train_img_shuffle[i][0][window_len][0] ))\n",
    "    for j in range (ch_number):\n",
    "        train_img_2[i][j] = np.delete(train_img_shuffle[i][j], window_len, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 152
    },
    "colab_type": "code",
    "id": "3aY39Gq91gkv",
    "outputId": "b6e98a67-eeb4-4db4-ddf9-195c62f75cc0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 64, 20, 160)\n",
      "(?, 32, 10, 160)\n",
      "(?, 30, 8, 128)\n",
      "(?, 15, 4, 128)\n",
      "(?, 13, 2, 256)\n",
      "(?, ?)\n",
      "(?, 512)\n",
      "(?, 20)\n"
     ]
    }
   ],
   "source": [
    "# Create Model\n",
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import regularizers\n",
    "from keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "\n",
    "train_label = to_categorical(train_label, subject_number)\n",
    "test_label = to_categorical(test_label, subject_number)\n",
    "\n",
    "x_train =train_img_2[:20000]\n",
    "y_train =train_label[:20000]\n",
    "\n",
    "x_valid =train_img_2[20000:]\n",
    "y_valid =train_label[20000:]\n",
    "\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(64, (3,3), activation = 'relu', padding='same', input_shape = (ch_number,window_len,160), data_format= \"channels_first\" ))\n",
    "print(model.output.shape)\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "print(model.output.shape)\n",
    "model.add(layers.Conv2D(128, (3,3), activation = 'relu'))\n",
    "print(model.output.shape)\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "print(model.output.shape)\n",
    "model.add(layers.Conv2D(256, (3,3), activation = 'relu'))\n",
    "print(model.output.shape)\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "print(model.output.shape)\n",
    "model.add(layers.Dense(512, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "print(model.output.shape)\n",
    "model.add(layers.Dense(subject_number, activation = 'softmax'))\n",
    "print(model.output.shape)\n",
    "\n",
    "model.compile(loss= 'categorical_crossentropy',\n",
    "              optimizer= optimizers.RMSprop(lr= 1e-4),\n",
    "              metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROHnPvn91jAl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 15140 samples\n",
      "Epoch 1/20\n",
      "  340/20000 [..............................] - ETA: 11:39 - loss: 2.9798 - acc: 0.0559"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cd71e94ad381>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;32m~/anaconda3/envs/mne/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda3/envs/mne/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mne/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mne/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/mne/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training\n",
    "history = model.fit(\n",
    "        x_train,\n",
    "        y_train,\n",
    "        epochs = 20,\n",
    "        batch_size = 20,\n",
    "        validation_data = (x_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5i7PMlbT96iD",
    "outputId": "7c84b3e2-4d08-44e7-b9df-812e6676097e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  960/11700 [=>............................] - ETA: 2:53"
     ]
    }
   ],
   "source": [
    "model.evaluate(test_img, test_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "QPJdvMzB1o74",
    "outputId": "836c6787-0807-4eb7-a918-6e977dd9b0d4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history \n",
    "\n",
    "loss_values = history_dict ['loss'] \n",
    "\n",
    "val_loss_values = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(epochs, loss_values, 'bo', label='Training loss') \n",
    "\n",
    "plt.plot(epochs, val_loss_values, 'b', label='Validation loss') \n",
    "\n",
    "plt.title('Training and validation loss') \n",
    "\n",
    "plt.xlabel('Epochs') \n",
    "\n",
    "plt.ylabel('Loss') \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "Es1vGaRAPI-y",
    "outputId": "a692d5b0-d930-4435-d078-8d5e6a19cbb1"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history \n",
    "\n",
    "acc_values = history_dict ['acc'] \n",
    "\n",
    "val_acc_values = history_dict['val_acc']\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(epochs, acc_values, 'bo', label='Training acc') \n",
    "\n",
    "plt.plot(epochs, val_acc_values, 'b', label='Validation acc') \n",
    "\n",
    "plt.title('Training and validation acc') \n",
    "\n",
    "plt.xlabel('Epochs') \n",
    "\n",
    "plt.ylabel('acc') \n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hyFYHNEVPL2M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "1 ch(10. Cz).ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
